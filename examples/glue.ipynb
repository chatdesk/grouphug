{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23eedacc-0041-41a6-b9a2-15a6e74a495b",
   "metadata": {},
   "source": [
    "# GLUE training\n",
    "This notebook shows how to fine-tune a model on *all* glue tasks simultaneously, including evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bce55a-54f1-453f-b50a-06eae814fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # ensure we can run examples as-is in the package's poetry env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ab1f8-4d91-40e1-b6ee-6201ff8d91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset, load_metric\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, TrainingArguments\n",
    "\n",
    "from grouphug import AutoMultiTaskModel, ClassificationHeadConfig, DatasetFormatter, LMHeadConfig, MultiTaskTrainer\n",
    "from grouphug.config import logger\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ddc31-8083-4b1f-a79a-afe797192174",
   "metadata": {},
   "source": [
    "## Define which model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e01fe-cd49-4429-8aa5-29c0c0759c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.logging.set_verbosity_info()  # uncomment for more logging\n",
    "base_model = \"HannahRoseKirk/Hatemoji\"  # a deberta model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945ae0f-bdce-4470-a522-72db654eeeda",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010ac53-5173-441f-ac55-8abb1fd0920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),  # is this sentence grammatical?\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),  # label as neutral, entailment, contradiction\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),  # whether the sentences in the pair are semantically equivalent.\n",
    "    \"qnli\": (\"question\", \"sentence\"),  # whether the context sentence contains the answer to the question\n",
    "    \"qqp\": (\"question1\", \"question2\"),  # determine whether a pair of questions are semantically equivalent.\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),  # similar to mnli\n",
    "    \"sst2\": (\"sentence\", None),  # sentiment\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),  # similarity score from 0 to 5.\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),  # entailment\n",
    "}\n",
    "tasks = list(task_to_keys.keys())\n",
    "\n",
    "\n",
    "def load_and_rename(task, reduce_size_target=None):\n",
    "    k1, k2 = task_to_keys[task]\n",
    "    dataset = load_dataset(\"glue\", task).rename_column(\"label\", task)\n",
    "\n",
    "    if k2 is not None:\n",
    "        dataset = dataset.rename_column(k1, \"text1\").rename_column(k2, \"text2\")\n",
    "    else:\n",
    "        dataset = dataset.rename_column(k1, \"text\")\n",
    "\n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": dataset[\"train\"],\n",
    "            \"validation\": concatenate_datasets([v for k, v in dataset.items() if k.startswith(\"validation\")]),\n",
    "            \"test\": concatenate_datasets([v for k, v in dataset.items() if k.startswith(\"test\")]),\n",
    "        }\n",
    "    )\n",
    "    test_labels = dataset[\"test\"].unique(task)\n",
    "    if reduce_size_target:\n",
    "        for k, target_size in reduce_size_target.items():\n",
    "            dataset[k] = Dataset.from_dict(dataset[k][:target_size])\n",
    "            logger.debug(f\"Reducing sizes to {len(dataset[k])} for {k}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4a285-bf54-4d1d-8611-8215f3a6e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = {\"train\": 2000, \"validation\": 100}  # just to keep it quick\n",
    "glue_data = {task: load_and_rename(task, target_size) for task in tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd65faa-1899-4050-b463-40f6f519a38f",
   "metadata": {},
   "source": [
    "## Define tokenizer and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee7e9a-ea2b-45b7-a15f-531964c66c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "fmt = DatasetFormatter().tokenize(max_length=512).tokenize((\"text1\", \"text2\"), max_length=512)\n",
    "data = fmt.apply(glue_data, tokenizer=tokenizer, splits=[\"train\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99009c76-4403-4bb7-8489-eb200f46ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_configs = [ClassificationHeadConfig.from_data(data, task, detached=False, ignore_index=-1) for task in tasks]\n",
    "# We fine-tune directly on masked inputs. This works well in practice, but may not work well when single words are very important like Cola.\n",
    "head_configs += [LMHeadConfig(weight=0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77046b9-985d-40fe-96ef-7a0f7b09d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoMultiTaskModel.from_pretrained(base_model, head_configs, formatter=fmt, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57802e28-f4ae-4f83-a644-975156b127da",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../output/demo\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19e216-0e85-4341-9274-879268139cf7",
   "metadata": {},
   "source": [
    "## Define metrics function\n",
    "Note additional arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb475a-0581-4c56-823b-521ac39282dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds, dataset_name, heads):\n",
    "    metrics_f = load_metric(\"glue\", dataset_name)\n",
    "    logits, labels = eval_preds\n",
    "    if dataset_name == \"stsb\":\n",
    "        return metrics_f.compute(predictions=logits, references=labels)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metrics_f.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a224e23-12c4-4e1b-a9f2-ed8567fc1517",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0bcb8-e1d9-4cba-8967-87aadd04584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_data=data[:, \"train\"],\n",
    "    eval_data=data[:, \"validation\"],\n",
    "    eval_heads={t: [t] for t in tasks},  # for dataset [key], run heads [value]\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a6b56-bf23-42c9-bb35-d5b93567699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfa1be-178a-473a-b541-5b46f08c95db",
   "metadata": {},
   "source": [
    "## The model predict function takes dicts or entire datasets and preprocesses, infers, and maps back to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23dea5-4597-45e5-bf5d-42096dc1441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict({\"text\": \"The quick brown fox jumped over the lazy dog!\"})[\"cola_predicted_label\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
