{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23eedacc-0041-41a6-b9a2-15a6e74a495b",
   "metadata": {},
   "source": [
    "# Sentiment training\n",
    "This notebook shows how to fine-tune a model on a few different sentiment datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bddf81-1d84-4874-9374-8fef35fdf10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # ensure we can run examples as-is in the package's poetry env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdd4dc-1e3d-47d4-872a-973d7606a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset, load_metric\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, TrainingArguments\n",
    "\n",
    "from grouphug import AutoMultiTaskModel, ClassificationHeadConfig, DatasetFormatter, LMHeadConfig, MultiTaskTrainer\n",
    "from grouphug.config import logger\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ddc31-8083-4b1f-a79a-afe797192174",
   "metadata": {},
   "source": [
    "## Define which model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e01fe-cd49-4429-8aa5-29c0c0759c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.logging.set_verbosity_info()  # uncomment for more logging\n",
    "base_model = \"prajjwal1/bert-tiny\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945ae0f-bdce-4470-a522-72db654eeeda",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cea71-fc50-474b-a80d-3e88d0d694fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_data = load_dataset(\"IsaacBot/GP-Sentiment\").rename_column(\"content\", \"text\")\n",
    "imdb_data = load_dataset(\"imdb\").rename_column(\"label\", \"negpos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668aa27c-edbd-4d42-85e8-a49c228e0b4f",
   "metadata": {},
   "source": [
    "## Define tokenizer and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a68e4-06ab-4035-9705-7c20e6e0e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "fmt = DatasetFormatter().tokenize(max_length=512).encode('score')\n",
    "data = fmt.apply({\"gp\": gp_data, \"imdb\": imdb_data}, tokenizer=tokenizer, splits=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd65faa-1899-4050-b463-40f6f519a38f",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99009c76-4403-4bb7-8489-eb200f46ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_configs = (\n",
    "    [  # as labels are different, we create different classifier heads for each task, but the base model is shared\n",
    "        ClassificationHeadConfig.from_data(data, \"score\", classifier_hidden_size=50),\n",
    "        ClassificationHeadConfig.from_data(data, \"negpos\", classifier_hidden_size=20, weight=2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77046b9-985d-40fe-96ef-7a0f7b09d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoMultiTaskModel.from_pretrained(base_model, head_configs, formatter=fmt, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a224e23-12c4-4e1b-a9f2-ed8567fc1517",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0bcb8-e1d9-4cba-8967-87aadd04584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../output/demo\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_data=data[:, \"train\"],\n",
    "    eval_data=data[:, \"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc551e-e7f4-4c47-aa18-2e047ac8fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfa1be-178a-473a-b541-5b46f08c95db",
   "metadata": {},
   "source": [
    "## The model predict function takes dicts or entire datasets and preprocesses, infers, and maps back to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23dea5-4597-45e5-bf5d-42096dc1441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict({\"text\": \"This will predict both things at once, giving probabilities, labels, and predicted ids. Awesome!\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
