{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b50547-7eca-48e4-b73d-bb046ac882aa",
   "metadata": {},
   "source": [
    "# This demo tests the effect of different language modelling heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96963a2d-60ce-4326-b1e6-18c4c64cba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # ensure we can run examples as-is in the package's poetry env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f7a95-1b1d-476d-a7a5-420fa28926c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "import torch\n",
    "from grouphug import AutoMultiTaskModel, ClassificationHeadConfig, DatasetFormatter, LMHeadConfig, MultiTaskTrainer\n",
    "\n",
    "from utils import compute_classification_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dbcdcf-3942-4f6e-a61b-21e8ad428f47",
   "metadata": {},
   "source": [
    "## A basic modelling task similar to the readme example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106748fe-e755-4017-870e-43c168f21c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_emotion = load_dataset(\"tweet_eval\",\"emotion\").rename_column(\"label\", \"emotion\")\n",
    "\n",
    "base_model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "formatter = DatasetFormatter().tokenize()\n",
    "data = formatter.apply(tweet_emotion, tokenizer=tokenizer)\n",
    "\n",
    "head_configs = [ClassificationHeadConfig.from_data(data, \"emotion\", classifier_hidden_size=32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034c437-2b9c-4c94-83bf-813c56fcb822",
   "metadata": {},
   "source": [
    "## Adding different LM heads to a classification task and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf39639-bc5c-411b-9a79-364d48b2fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lm_heads = {\n",
    "    \"none\": [],\n",
    "    \"mlm\": [LMHeadConfig(weight=0.2)],\n",
    "    \"mtd\": [LMHeadConfig(masked_token_detection=True,weight=0.2)],\n",
    "    \"mlm+mtd\": [LMHeadConfig(masked_language_modelling=True, masked_token_detection=True,weight=0.2)],\n",
    "}\n",
    "results = {}\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "for test_key, lm_head in test_lm_heads.items():\n",
    "    model = AutoMultiTaskModel.from_pretrained(\n",
    "        base_model, head_configs + lm_head, formatter=formatter, tokenizer=tokenizer\n",
    "    )\n",
    "    trainer = MultiTaskTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_data=data[:, \"train\"],\n",
    "        eval_data=data[:, \"test\"],\n",
    "        eval_heads=[\"emotion\"],\n",
    "        compute_metrics=compute_classification_metrics,\n",
    "        args=training_args,\n",
    "    )\n",
    "    trainer.train()\n",
    "    results[test_key] = pd.DataFrame(trainer.state.log_history)\n",
    "    model = None\n",
    "    trainer = None\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c86d24-43dc-43d0-b6da-40c6bdf3e0bd",
   "metadata": {},
   "source": [
    "## Inspecting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5de52-1244-4f0a-a6cf-27614e27092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,5))\n",
    "for i, k in enumerate(['loss','eval_loss','eval_emotion_f1','eval_emotion_matthews_correlation']):\n",
    "    for test_name, df in results.items():\n",
    "        ax = plt.subplot(1,4,i+1)\n",
    "        df.dropna(subset=k).plot(x='step',y=k,ax=ax)\n",
    "    plt.legend(results.keys())\n",
    "    plt.title(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
